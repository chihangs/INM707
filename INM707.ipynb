{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42526f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from abc import ABC, abstractmethod\n",
    "from random import random\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#initialize for random seeds/states\n",
    "tf.keras.backend.clear_session()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "rng = np.random.default_rng(12345)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf663d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class paper_game(ABC):\n",
    "    def __init__(self, start_state):\n",
    "        self.state=start_state\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transition(state, action, player):\n",
    "        #return next state\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reward(self, state, action, player):\n",
    "        #return reward immediately after action\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def policy(self, state):\n",
    "        #return action\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Q(self, state, action):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Q_update(self, alpha, gamma):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e48ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tit_tac_toe(paper_game):\n",
    "#state: 3x3 array to represent board config, with 1 representing self-checker, -1 representing opponent-checker, 0 representing empty\n",
    "    def __init__(self, start_state=np.zeros((3,3), dtype=np.int8), epsilon=0.9, alpha=0.5, gamma=0.8):\n",
    "        self.state=start_state\n",
    "        self.epsilon = epsilon\n",
    "        self.aplha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((19683, 9))  #initialize Q matrix with state 'reference' (see ref function) and action key (see key function)    \n",
    "    \n",
    "    def transition(state, action, player):\n",
    "        #action: a list of row and column index to indicate which cell is chosen\n",
    "        #player: 1 or -1\n",
    "        #applicable to both tic_tac_toe and Connect-4\n",
    "        #return next state\n",
    "        new_state = copy.deepcopy(state)\n",
    "        new_state[action[0]][action[1]] = player\n",
    "        return new_state\n",
    "    \n",
    "    def win_status(s):\n",
    "        #s: state, \n",
    "        #return: whether win or not\n",
    "        win = 3 in np.sum(s, axis=0) or 3 in np.sum(s, axis=1) or np.sum(s.diagonal())==3 or np.sum(np.fliplr(s).diagonal())==3\n",
    "        return win\n",
    "    \n",
    "    def ref(state):\n",
    "        #change state matrix into vector\n",
    "        ref = np.dot((state + 1).flatten() , np.matrix([1,3,3**2,3**3,3**4,3**5,3**6,3**7,3**8]))\n",
    "        return ref\n",
    "    \n",
    "    def key(action):\n",
    "        #change actions [r, c] into numbers 0-8\n",
    "        return 3*action[0]+action[1]\n",
    "    \n",
    "    def action_list(state):\n",
    "        act_list = np.where(state==0)\n",
    "        return act_list\n",
    "\n",
    "    def reward(self, state, action, player):\n",
    "        s = player * self.transition(state, action, player)\n",
    "        if self.win_status(s):\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "    \n",
    "    def Q(self, state, action):\n",
    "        Q = self.Q[self.ref(state)][self.key(action)]\n",
    "        return Q\n",
    "\n",
    "    def Q_update(self, state, action, reward_1, maxQ, player):\n",
    "        #state of board instead of current player's perspective\n",
    "        #reward_1: reward of player 1 regardless of who is current player\n",
    "        #maxQ: player's maxQ after another action\n",
    "        #player: 1 or -1\n",
    "        s = player*state\n",
    "        self.Q[self.ref(s)][self.key(action)] += self.alpha*(reward_1*player + self.gamma*maxQ - self.Q[self.ref(s)][self.key(action)] )\n",
    "\n",
    "\n",
    "    def random_move(self, state):\n",
    "        act_list = self.action_list(state)\n",
    "        i = np.random.randint(len(act_list[0]))\n",
    "        action = [act_list[0][i],act_list[1][i]]\n",
    "        return action\n",
    "\n",
    "    def best_move(self, state):\n",
    "        act_list = self.action_list(state)\n",
    "        maxQ = -10000\n",
    "        for i in range (len(act_list[0])):\n",
    "            a = [act_list[0][i],act_list[1][i]]\n",
    "            temp = self.Q(state, a)\n",
    "            if temp > maxQ:\n",
    "                maxQ = temp\n",
    "                action = a\n",
    "        return action, maxQ\n",
    "    \n",
    "    def policy(self, state):\n",
    "        exploit = (not self.train) or rng.random()> self.epsilon\n",
    "        if exploit:\n",
    "            action, maxQ = self.best_move(state)\n",
    "        else:\n",
    "            action = self.random_move(state)\n",
    "        if self.epsilon < 0.5:\n",
    "            self.epsilon *= 0.9999\n",
    "        else:\n",
    "            self.epsilon *= 0.99999    \n",
    "        return action\n",
    "    \n",
    "    def train(self, train_number=2000):\n",
    "        self.train = True\n",
    "        #loop by train_number\n",
    "        for i in range (train_number):\n",
    "            states =[]\n",
    "            state = self.state #start state\n",
    "            states.append(state)\n",
    "            actions=[]\n",
    "            r1=[0] #reward list at different times for player 1; for player -1: use zero sum property\n",
    "            t = 0\n",
    "            #choose who is X, i.e. plays first\n",
    "            player = 1 #as this is self-play, we simply assume 1 always plays first and be playerX\n",
    "            endgame = False\n",
    "            while not endgame:\n",
    "                #update his Q before action if previous has action\n",
    "                s = player * state\n",
    "                if t >=2:\n",
    "                    action, maxQ = self.best_move(s)\n",
    "                    self.Q_update(states[t-2], actions[t-2], r1[t], maxQ, player)\n",
    "                    \n",
    "                #action\n",
    "                action = self.policy(s)\n",
    "                actions.append(action)\n",
    "                t += 1\n",
    "                r1.append(player*self.reward(state, action, player))\n",
    "                state=self.transition(state, action, player)\n",
    "                states.append(state)\n",
    "                #check win status and end game status, update Q for both players if end game\n",
    "                endgame = self.win_status(s) or t==9\n",
    "                if endgame:\n",
    "                    self.Q_update(states[t-1], actions[t-1], r1[t], 0, player)\n",
    "                    self.Q_update(states[t-2], actions[t-2], r1[t], 0, -player)\n",
    "                    \n",
    "                #update player for next loop\n",
    "                player *= -1\n",
    "        self.last_episode = states\n",
    "        \n",
    "    def display_board(state):\n",
    "        print('-------------------')\n",
    "        print(state)\n",
    "        print('-------------------')\n",
    "\n",
    "    def display_episode(self):\n",
    "        for i in range(len(self.last_episode)):\n",
    "            print('state '+i+' :')\n",
    "            self.display_board(self.last_episode[i])\n",
    "            \n",
    "    def play():\n",
    "        self.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7853be62",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "action_list() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_56902/3253455588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtit_tac_toe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_board\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_56902/829392894.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_number)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;31m#action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_56902/829392894.py\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.9999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_56902/829392894.py\u001b[0m in \u001b[0;36mrandom_move\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrandom_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mact_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mact_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: action_list() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "test1 = tit_tac_toe()\n",
    "test1.train(10)\n",
    "test1.display_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros((3,3))\n",
    "np.where(s == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bfb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.ones((3,3))\n",
    "np.where(s == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.where(s == 0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28018fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(test1.Q == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test1.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804231bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
