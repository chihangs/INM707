{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4e71145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from abc import ABC, abstractmethod\n",
    "from random import random\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tkinter as tk\n",
    "#initialize for random seeds/states\n",
    "tf.keras.backend.clear_session()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "rng = np.random.default_rng(12345)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7322d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class paper_game(ABC):\n",
    "    def __init__(self, start_state):\n",
    "        self.state=start_state\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transition(state, action, player):\n",
    "        #return next state\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def reward(self, state, action, player):\n",
    "        #return reward immediately after action\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def policy(self, state):\n",
    "        #return action\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Q_func(self, state, action):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Q_update(self, state, action, reward_1, maxQ, player):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "878fa879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tit_tac_toe(paper_game):\n",
    "#state: 3x3 array to represent board config, with 1 representing self-checker, -1 representing opponent-checker, 0 representing empty\n",
    "    def __init__(self, start_state=np.zeros((3,3), dtype=np.int8), epsilon=0.9, alpha=0.5, gamma=0.8):\n",
    "        self.state=start_state\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros((19683, 9))  #initialize Q matrix with state 'reference' (see ref function) and action key (see key function)    \n",
    "    \n",
    "    def transition(self, state, action, player):\n",
    "        #action: a list of row and column index to indicate which cell is chosen\n",
    "        #player: 1 or -1\n",
    "        #applicable to both tic_tac_toe and Connect-4\n",
    "        #return next state\n",
    "        new_state = copy.deepcopy(state)\n",
    "        new_state[action[0]][action[1]] = player\n",
    "        return new_state\n",
    "    \n",
    "    def win_status(self, s):\n",
    "        #s: state, \n",
    "        #return: whether win or not\n",
    "        win = 3 in np.sum(s, axis=0) or 3 in np.sum(s, axis=1) or np.sum(s.diagonal())==3 or np.sum(np.fliplr(s).diagonal())==3\n",
    "        return win\n",
    "\n",
    "    def lose_status(self, s):\n",
    "        #s: state, \n",
    "        #return: whether win or not\n",
    "        lose = -3 in np.sum(s, axis=0) or -3 in np.sum(s, axis=1) or np.sum(s.diagonal())==-3 or np.sum(np.fliplr(s).diagonal())==-3\n",
    "        return lose\n",
    "    \n",
    "    def ref(self, state):\n",
    "        #change state matrix into vector\n",
    "        ref = np.sum(np.matrix([1,3,3**2,3**3,3**4,3**5,3**6,3**7,3**8]) @ np.reshape((state + 1).flatten(),(9,1)))\n",
    "        return ref\n",
    "    \n",
    "    def key(self, action):\n",
    "        #change actions [r, c] into numbers 0-8\n",
    "        return 3*action[0]+action[1]\n",
    "    \n",
    "    def action_list(self, state):\n",
    "        act_list = np.where(state==0)\n",
    "        return act_list\n",
    "\n",
    "    def reward(self, state, action, player):\n",
    "        s = player * self.transition(state, action, player)\n",
    "        if self.win_status(s):\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "    \n",
    "    def Q_func(self, state, action):\n",
    "        Q = self.Q[self.ref(state)][self.key(action)]\n",
    "        return Q\n",
    "\n",
    "    def Q_update(self, state, action, reward_1, maxQ, player):\n",
    "        #state of board instead of current player's perspective\n",
    "        #reward_1: reward of player 1 regardless of who is current player\n",
    "        #maxQ: player's maxQ after another action\n",
    "        #player: 1 or -1\n",
    "        s = player*state\n",
    "        self.Q[self.ref(s)][self.key(action)] += self.alpha*(reward_1*player + self.gamma*maxQ - self.Q[self.ref(s)][self.key(action)] )\n",
    "\n",
    "\n",
    "    def random_move(self, state):\n",
    "        act_list = self.action_list(state)\n",
    "        i = np.random.randint(len(act_list[0]))\n",
    "        action = [act_list[0][i],act_list[1][i]]\n",
    "        return action\n",
    "\n",
    "    def best_move(self, state):\n",
    "        act_list = self.action_list(state)\n",
    "        maxQ = -10000\n",
    "        for i in range (len(act_list[0])):\n",
    "            a = [act_list[0][i],act_list[1][i]]\n",
    "            temp = self.Q_func(state, a)\n",
    "            if temp > maxQ:\n",
    "                maxQ = temp\n",
    "                action = a\n",
    "        return action, maxQ\n",
    "    \n",
    "    def policy(self, state):\n",
    "        exploit = (not self.train) or rng.random()> self.epsilon or self.play\n",
    "        if exploit:\n",
    "            action, maxQ = self.best_move(state)\n",
    "        else:\n",
    "            action = self.random_move(state)\n",
    "        if self.epsilon < 0.5:\n",
    "            self.epsilon *= 0.9999\n",
    "        else:\n",
    "            self.epsilon *= 0.99999    \n",
    "        return action\n",
    "    \n",
    "    def train(self, train_number=2000):\n",
    "        self.train = True\n",
    "        self.play = False\n",
    "        #loop by train_number\n",
    "        start_time = time.time()\n",
    "        for i in range (train_number):\n",
    "            if i % 10000 == 0 and i != 0:\n",
    "                end_time = time.time()\n",
    "                elp_time = '{:.2f}'.format(end_time - start_time)\n",
    "                print(f'Training Phase, epoch {i}, elapsed time:{elp_time}')\n",
    "                start_time = time.time()\n",
    "                \n",
    "            states =[]\n",
    "            state = self.state #start state\n",
    "            states.append(state)\n",
    "            actions=[]\n",
    "            r1=[0] #reward list at different times for player 1; for player -1: use zero sum property\n",
    "            t = 0\n",
    "            #choose who is X, i.e. plays first\n",
    "            player = 1 #as this is self-play, we simply assume 1 always plays first and be playerX\n",
    "            endgame = False\n",
    "            while not endgame:\n",
    "                #update his Q before action if previous has action\n",
    "                s = player * state\n",
    "                if t >=2:\n",
    "                    action, maxQ = self.best_move(s)\n",
    "                    self.Q_update(states[t-2], actions[t-2], r1[t], maxQ, player)\n",
    "                    \n",
    "                #action\n",
    "                action = self.policy(s)\n",
    "                actions.append(action)\n",
    "                t += 1\n",
    "                r1.append(player*self.reward(state, action, player))\n",
    "                state=self.transition(state, action, player)\n",
    "                states.append(state)\n",
    "                #check win status and end game status, update Q for both players if end game\n",
    "                endgame = self.win_status(s) or t==9\n",
    "                if endgame:\n",
    "                    self.Q_update(states[t-1], actions[t-1], r1[t], 0, player) #r1[t+1] is not used as there won't be t+1 when endgame is reached\n",
    "                    self.Q_update(states[t-2], actions[t-2], r1[t], 0, -player)\n",
    "                    \n",
    "                #update player for next loop\n",
    "                player *= -1\n",
    "        self.last_episode = states\n",
    "        \n",
    "    def display_board(self, state):\n",
    "        print('-------------------')\n",
    "        print(state)\n",
    "        print('-------------------')\n",
    "\n",
    "    def display_episode(self):\n",
    "        for i in range(len(self.last_episode)):\n",
    "            print('state {} :'.format(i))\n",
    "            self.display_board(self.last_episode[i])\n",
    "            \n",
    "    def play_game(self,starter = 'player'):\n",
    "        self.play = True\n",
    "        self.train = False\n",
    "        state = np.zeros((3,3), dtype=np.int8)\n",
    "        \n",
    "        endgame = False\n",
    "        t = 0\n",
    "        self.display_board(state)\n",
    "        while not endgame:\n",
    "            if starter == 'player':\n",
    "                i = int(input('enter row:\\n'))\n",
    "                j = int(input('enter column:\\n'))\n",
    "                state[i,j] = -1\n",
    "\n",
    "            else:\n",
    "                i,j = self.best_move(state)[0]\n",
    "                state[i,j] = 1\n",
    "\n",
    "            self.display_board(state)   \n",
    "            starter *= -1\n",
    "            endgame = self.win_status(state) or self.lose_status(state) or t==9\n",
    "            t += 1\n",
    "                    \n",
    "   \n",
    "    def play_gui(self):\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "791a7d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Phase, epoch 10000, elapsed time:30.13\n",
      "Training Phase, epoch 20000, elapsed time:49.63\n",
      "Training Phase, epoch 30000, elapsed time:48.78\n",
      "Training Phase, epoch 40000, elapsed time:49.71\n",
      "Training Phase, epoch 50000, elapsed time:48.23\n",
      "Training Phase, epoch 60000, elapsed time:45.53\n",
      "Training Phase, epoch 70000, elapsed time:49.03\n",
      "Training Phase, epoch 80000, elapsed time:46.17\n",
      "Training Phase, epoch 90000, elapsed time:44.71\n"
     ]
    }
   ],
   "source": [
    "test1 = tit_tac_toe()\n",
    "test1.train(100000)\n",
    "# test1.display_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f569a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "-------------------\n",
      "enter row0\n",
      "enter column0\n",
      "-------------------\n",
      "[[-1  0  0]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]]\n",
      "-------------------\n",
      "-------------------\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "-------------------\n",
      "enter row1\n",
      "enter column2\n",
      "-------------------\n",
      "[[-1  0  0]\n",
      " [ 0  1 -1]\n",
      " [ 0  0  0]]\n",
      "-------------------\n",
      "-------------------\n",
      "[[-1  1  0]\n",
      " [ 0  1 -1]\n",
      " [ 0  0  0]]\n",
      "-------------------\n",
      "enter row2\n",
      "enter column2\n",
      "-------------------\n",
      "[[-1  1  0]\n",
      " [ 0  1 -1]\n",
      " [ 0  0 -1]]\n",
      "-------------------\n",
      "-------------------\n",
      "[[-1  1  0]\n",
      " [ 0  1 -1]\n",
      " [ 0  1 -1]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "test1.play_game('player')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "684a7a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "-------------------\n",
      "enter row1\n",
      "enter column1\n",
      "-------------------\n",
      "[[ 0  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "-------------------\n",
      "-------------------\n",
      "[[ 1  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "-------------------\n",
      "enter row2\n",
      "enter column2\n",
      "-------------------\n",
      "[[ 1  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0 -1]]\n",
      "-------------------\n",
      "-------------------\n",
      "[[ 1  0  1]\n",
      " [ 0 -1  0]\n",
      " [ 0  0 -1]]\n",
      "-------------------\n",
      "enter row2\n",
      "enter column0\n",
      "-------------------\n",
      "[[ 1  0  1]\n",
      " [ 0 -1  0]\n",
      " [-1  0 -1]]\n",
      "-------------------\n",
      "-------------------\n",
      "[[ 1  1  1]\n",
      " [ 0 -1  0]\n",
      " [-1  0 -1]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "test1.play_game('palyer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
