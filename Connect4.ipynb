{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9835f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from abc import ABC, abstractmethod\n",
    "from random import random, randint, sample\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tkinter as tk\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c2786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize for random seeds/states\n",
    "seeds=[12345,42,42,42]\n",
    "def randomize(seed_rng=seeds[0], seed_np=seeds[1], seed_torch=seeds[2], seed_tf=seeds[3]):\n",
    "    tf.keras.backend.clear_session()\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    rng = np.random.default_rng(seed_rng)\n",
    "    np.random.seed(seed_np)\n",
    "    torch.manual_seed(seed_torch)\n",
    "    tf.random.set_seed(seed_tf)\n",
    "\n",
    "randomize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e8beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class paper_game(ABC):\n",
    "    def __init__(self, start_state):\n",
    "        self.state=start_state\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transition(state, action, player):\n",
    "        #return next state\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104ca979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class connect4(paper_game):\n",
    "    #state: 6x7 array to represent board config, with 1 representing self-checker, -1 representing opponent-checker, 0 representing empty\n",
    "    def __init__(self, start_state=np.zeros((6,7), dtype=np.int8), epsilon=0.9, alpha=0.5, gamma=0.8):\n",
    "        self.first_state = self.state = start_state\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.player_turn = 1\n",
    "        self.turn_count = 0\n",
    "        self.Q = None\n",
    "        #self.Q = np.zeros((19683, 9)) - need to change to Q-network somewhere\n",
    "        \n",
    "    def transition(self, state, action, player):\n",
    "        #action: a list/tuple of row and column index to indicate which cell is chosen\n",
    "        #player: 1 or -1, with 1 being self-checker, -1 being opponent-checker\n",
    "        #applicable to both tic_tac_toe and Connect-4\n",
    "        #return next state\n",
    "        new_state = copy.deepcopy(state)\n",
    "        new_state[action[0]][action[1]] = player\n",
    "        return new_state      \n",
    "\n",
    "    def win_status(self, s, action, player):  \n",
    "        #note: unlike tic-tac-toe, action is needed for input, to save computation\n",
    "        #suppose state before action is not winning, so only check lines connected with action\n",
    "        #s: state before action\n",
    "        #action: list/tuple/array, unpackable to row and column index to indicate which cell is chosen\n",
    "        #player: 1 or -1, the player taking the action\n",
    "        #----return----- \n",
    "        #win: whether player wins or not with the action\n",
    "        #lose means \"if\" it is opposite player's turn instead, hypothetical only\n",
    "        w = 3 * player  # use 3 instead of 4 because the action is not yet put into board\n",
    "        r, c = action\n",
    "        f = np.fliplr(s) #flip for finding diagonal in opposite direction, i.e. upwards\n",
    "        c_f = 6 - c      #for use in flipped states\n",
    "        lines = [s[r:r+4, c].sum()] # vertical line\n",
    "        for i in range(4):\n",
    "            lines.append(s[r,  c-i:c+4-i].sum()) #horizontal line\n",
    "            lines.append(s[r-i:r+4-i, c-i:c+4-i].diagonal().sum()) #diagonal line\n",
    "            lines.append(f[r-i:r+4-i, c_f-i:c_f+4-i].diagonal().sum()) #diagonal line in opposite direction\n",
    "        win = w in lines\n",
    "        lose = -w in lines\n",
    "        return win, lose   #lose is only hypothetical check here\n",
    "    \n",
    "    def action_list(self, state):\n",
    "        #return array of shape (n, 2), where n is no of possible actions, each represented by row and column\n",
    "        row_list = 5 - np.sum(abs(state), axis=0)\n",
    "        col_list = np.where(row_list >= 0)\n",
    "        act_list = np.squeeze(np.dstack((row_list[col_list], col_list)))\n",
    "        return act_list    #note: unlike tic-tac-toe, act_list[0] is the first action instead of row indices\n",
    "    \n",
    "    def random_move(self, state, act_list=None): #unlike tic-tac-toe, it allows input of short-listed actions\n",
    "        #return action in array of shape (2,)\n",
    "        if act_list is None:\n",
    "            act_list = self.action_list(state)\n",
    "        i = np.random.randint(len(act_list))\n",
    "        action = act_list[i]\n",
    "        return action\n",
    "\n",
    "    def basic_check(self, state, player=1, act_list=None):       \n",
    "        #exploit immediate win / then avoid immediate win by opponent; otherwise return empty move\n",
    "        #act_list: allows input of short-listed actions of player to save computation\n",
    "        #--------return--------\n",
    "        #action: action that can win, or if not available then return action that blocks opponent's win\n",
    "        #win_count: dictionary, no. of ways to win immediately by each player\n",
    "        #safe_list: actions that opponent cannot win immediately \"above\", not consider win move or other moves that lose\n",
    "        #note: safe_list is list of array while act_list is array of array in action_list function\n",
    "        if act_list is None:\n",
    "            act_list = self.action_list(state)\n",
    "        action = []\n",
    "        block = []\n",
    "        safe_list = []\n",
    "        win_count = {1: 0, -1: 0,}\n",
    "        for a in act_list:\n",
    "            win, op_win = self.win_status(state, a, player)\n",
    "            if win:\n",
    "                action = a\n",
    "                win_count[player] += 1\n",
    "            if op_win:\n",
    "                block = a\n",
    "                win_count[-player] += 1   #neagtive player means opposite player\n",
    "            #check if there is trap i.e. whether opponent can win on the same column immediately above the move\n",
    "            if a[0] == 0: \n",
    "                safe_list.append(a)  #highest row has no danger above it\n",
    "            else:\n",
    "                s = self.transition(state, a, player=player)\n",
    "                #simulate opponent move immediately above, along same column\n",
    "                op_win, _ = self.win_status(s, [a[0]-1, a[1]], -player)  #note: opposite player, so win becomes op_win\n",
    "                if not op_win:\n",
    "                    safe_list.append(a)\n",
    "    \n",
    "        if len(action)==0:  #if action is still empty\n",
    "            action = block\n",
    "        return action, win_count, safe_list  #safe list does not consider win move or block move; it is a list of array, not array of array\n",
    "    \n",
    "    def basic_move(self, state, player=1):\n",
    "        #exploit immediate win and then avoid immediate win by opponent; otherwise random\n",
    "        #return action\n",
    "        action, win_count, act_list = self.basic_check(state, player=player)\n",
    "        if len(action)==0:  \n",
    "            if len(act_list)==0:  #short list from basic check can be empty if no safe space\n",
    "                act_list = self.action_list(state)\n",
    "            action = self.random_move(state, act_list)\n",
    "        return action\n",
    "    \n",
    "    def medium_move(self, state, player=1):\n",
    "        #exploit immediate win and immeidate checkmate and avoid such by opponent; otherwise random\n",
    "        #return action\n",
    "        action, win_count, act_list = self.basic_check(state, player=player)\n",
    "        if len(action)==0:  \n",
    "            if len(act_list)==0:  #short list from basic check can be empty if no safe space\n",
    "                act_list = self.action_list(state)\n",
    "            else:\n",
    "                safe_idx = []\n",
    "                for idx, a in enumerate(act_list): #note: this act_list is safe list from basic_check\n",
    "                    safe_idx.append(idx)\n",
    "                    s1 = self.transition(state, a, player=player)\n",
    "                    op_action, win_c1, op_safe_list = self.basic_check(s1, player=-player)\n",
    "                    if win_c1[player] > 1 or op_safe_list==[] or (win_c1[player]==1 and (op_action[1:2] not in np.array(op_safe_list)[:,1]) ):  #i.e. checkmate\n",
    "                        action = a\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        for op_act in op_safe_list:\n",
    "                            s2 = self.transition(state, op_act, player=-player)\n",
    "                            a2, win_c2, safe_list2 = self.basic_check(s2, player=player)\n",
    "                            if win_c2[player]==0:\n",
    "                                if  win_c2[-player]>1 or safe_list2==[] or (win_c2[-player]==1 and (a2[1:2] not in np.array(safe_list2)[:,1]))  :  #i.e. being checkmated\n",
    "                                    safe_idx.pop()\n",
    "                                    break\n",
    "                if safe_idx != []:\n",
    "                    act_list = np.array(act_list)\n",
    "                    act_list = act_list[safe_idx]\n",
    "        if len(action)==0:\n",
    "            action = self.random_move(state, act_list)\n",
    "        return action\n",
    "  \n",
    "    def display_board(self, state):\n",
    "        print('-------------------')\n",
    "        print(state)\n",
    "        print('-------------------')\n",
    "        print(np.array([[0,1,2,3,4,5,6]]))\n",
    "            \n",
    "    def display_colour(self, state):\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(state)\n",
    "        ax.set_xticks(np.arange(7))\n",
    "        ax.set_yticks(np.arange(6))\n",
    "        ax.set_xticklabels(np.arange(7))\n",
    "        ax.set_yticklabels(np.arange(6))\n",
    "        for i in range(6):\n",
    "            for j in range(7):\n",
    "                text = ax.text(j, i, state[i, j], ha=\"center\", va=\"center\", color=\"g\")\n",
    "        ax.set_title(\"1: computer; -1: you\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def play_game(self, starter = 1, policy_function=None):\n",
    "        state = np.zeros((6,7), dtype=np.int8)\n",
    "        player = starter\n",
    "        endgame = False\n",
    "        t = 0\n",
    "        if policy_function is None:\n",
    "            policy_function = self.medium_move \n",
    "        player_dict = {1: \"computer\", -1: \"you\",}\n",
    "        self.display_colour(state)\n",
    "        while not endgame:\n",
    "            if player == -1:\n",
    "                valid = False\n",
    "                while not valid:\n",
    "                    j = int(input('enter column: (0-6)\\n')) \n",
    "                    valid = j in [0,1,2,3,4,5,6]             #data validation on column inpu\n",
    "                    if not valid:\n",
    "                        print('Wrong column input!')\n",
    "                i = (5 - np.sum(abs(state),axis=0))[j]\n",
    "            else:\n",
    "                i, j = policy_function(state)\n",
    "            player_win, _ = self.win_status(state, [i,j], player)\n",
    "            state[i,j] = player\n",
    "            self.display_colour(state)  \n",
    "            if player_win:\n",
    "                print('The winner is '+player_dict[player])\n",
    "            player *= -1\n",
    "            t += 1\n",
    "            endgame = player_win or t==42\n",
    "        if not player_win:\n",
    "            print('This is a draw.')\n",
    "\n",
    "\n",
    "    def step(self,action,network):\n",
    "        #action is one of column numbers 0-6\n",
    "        #return state, reward, endgame\n",
    "        #in this function, the agent will play one step according to the game's state.\n",
    "        #Also, the rewards of each action would be saved in the replay memory.\n",
    "        reward = 0\n",
    "        endgame = False\n",
    "        self.turn_count += 1\n",
    "        j = action\n",
    "        i = (5 - np.sum(abs(self.state),axis=0))[j]\n",
    "        # self.state[i,j] = self.player_turn\n",
    "\n",
    "        if self.win_status(self.state,[i,j] ,1)[0]:\n",
    "            reward = 100\n",
    "            endgame = True\n",
    "            return_state = copy.deepcopy(self.state)\n",
    "            self.reset()\n",
    "            return return_state, reward, endgame\n",
    "\n",
    "        self.state[i,j] = 1\n",
    "        self.turn_count += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask = np.array([True if (i in self.action_list(state)[:,1]) else False for i in range(7)]).reshape(1,7)\n",
    "            j = (np.array(network(convert_state(self.state))) * mask).argmax()\n",
    "            i = (5 - np.sum(abs(self.state),axis=0))[j]\n",
    "\n",
    "        if self.win_status(self.state,[i,j] ,-1)[0]:\n",
    "            reward = -100\n",
    "            endgame = True\n",
    "            return_state = copy.deepcopy(self.state)\n",
    "            self.reset()\n",
    "            return return_state, reward, endgame\n",
    "\n",
    "        self.state[i,j] = -1\n",
    "        self.turn_count += 1\n",
    "\n",
    "        if self.turn_count == 42:\n",
    "            endgame = True\n",
    "            self.turn_count = 0\n",
    "            return_state = copy.deepcopy(self.state)\n",
    "            self.reset()\n",
    "            return return_state, reward, endgame\n",
    "\n",
    "        # if self.win_status(self.state,[i,j] ,self.player_turn)[0]:\n",
    "        #     reward = 100\n",
    "        #     endgame = True\n",
    "        #     return_state = copy.deepcopy(self.state)\n",
    "        #     self.reset()\n",
    "        #     return return_state, reward, endgame\n",
    "\n",
    "\n",
    "        # elif self.win_status(self.state,[i,j] ,self.player_turn)[1]:\n",
    "        #     reward = -100\n",
    "        #     endgame = True\n",
    "        #     return_state = copy.deepcopy(self.state)\n",
    "        #     self.reset()\n",
    "        #     return return_state, reward, endgame\n",
    "\n",
    "        # elif self.turn_count == 42:\n",
    "        #     endgame = True\n",
    "        #     self.turn_count = 0\n",
    "        #     return_state = copy.deepcopy(self.state)\n",
    "        #     self.reset()\n",
    "        #     return return_state, reward, endgame\n",
    "        # self.state[i,j] = 1\n",
    "        # self.state *= -1\n",
    "        # self.player_turn *= -1\n",
    "        # self.display_colour(self.state) \n",
    "        return self.state, reward, endgame\n",
    "\n",
    "    def win_p100(self,network):\n",
    "        #the agent would play 100 games with the random or medium moves\n",
    "        #and the result is the number of games that agent has won\n",
    "        self.Q = network\n",
    "        choices = [-1,1]*50\n",
    "        win_count = 0\n",
    "        for i in range(100):\n",
    "            state = np.zeros((6,7), dtype=np.int8)\n",
    "            current_turn = choices[-1]\n",
    "            endgame = False\n",
    "            t = 0\n",
    "            while not endgame:\n",
    "                if current_turn == 1:\n",
    "                    with torch.no_grad():\n",
    "                        mask = np.array([True if (i in self.action_list(state)[:,1]) else False for i in range(7)]).reshape(1,7)\n",
    "                        j = (np.array(self.Q(convert_state(state))) * mask).argmax()\n",
    "                    # j = self.Q(convert_state(state)).argmax()\n",
    "                    i = (5 - np.sum(abs(state),axis=0))[j]   \n",
    "                else:\n",
    "                    i,j = self.random_move(state)\n",
    "                    # i,j = self.medium_move(state,-1)      \n",
    "                \n",
    "                t += 1\n",
    "\n",
    "                if self.win_status(state,[i,j] ,1)[0]:\n",
    "                    win_count += 1\n",
    "                    endgame = True\n",
    "                elif self.win_status(state, [i,j], -1)[0]:\n",
    "                    endgame = True\n",
    "                elif t == 42:\n",
    "                    endgame = True\n",
    "                    \n",
    "                state[i,j] = current_turn\n",
    "                current_turn *= -1\n",
    "            choices = choices[:-1]\n",
    "        return win_count\n",
    "\n",
    "\n",
    "    def play_DQN(self,network):\n",
    "        #playing against the agent which decides each move based on a Q network\n",
    "\n",
    "        self.Q = network\n",
    "        state = np.zeros((6,7), dtype=np.int8)\n",
    "        current_turn = 1\n",
    "        endgame = False\n",
    "        t = 0\n",
    "        while not endgame:\n",
    "            if current_turn == 1:\n",
    "                with torch.no_grad():\n",
    "                    mask = np.array([True if (i in self.action_list(state)[:,1]) else False for i in range(7)]).reshape(1,7)\n",
    "                    j = (np.array(self.Q(convert_state(state))) * mask).argmax()\n",
    "                # j = self.Q(convert_state(state)).argmax()\n",
    "                i = (5 - np.sum(abs(state),axis=0))[j]\n",
    "\n",
    "            else:\n",
    "                j = int(input('enter column: (0-6)\\n'))\n",
    "                i = (5 - np.sum(abs(state),axis=0))[j]           \n",
    "\n",
    "            if self.win_status(state,[i,j] ,current_turn)[0]:\n",
    "                print(f'{current_turn} wins')\n",
    "                endgame = True\n",
    "\n",
    "            # elif self.win_status(state,[i,j] ,current_turn)[1]:\n",
    "            #     print(f'{current_turn} lose')\n",
    "            #     endgame = True\n",
    "\n",
    "            elif t == 42:\n",
    "                endgame = True\n",
    "                print('draw')\n",
    "                \n",
    "            state[i,j] = current_turn  \n",
    "            current_turn *= -1\n",
    "            t += 1\n",
    "            self.display_colour(state)\n",
    "\n",
    "    def reset(self):\n",
    "        #reset the games state to the initial values\n",
    "        self.state = np.zeros((6,7), dtype=np.int8)\n",
    "        self.player_turn = 1\n",
    "        self.turn_count = 0\n",
    "        return self.state\n",
    "        # self.display_colour(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f6f0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each memory of the game consists of the current state, the action of the agent, \n",
    "#the next state after the action, and the reward of that action.\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def convert_state(state):\n",
    "    #converting each state to a flatten tensor\n",
    "    #state = state.flatten()   #flatten inside NN instead if needed\n",
    "    state_tensor = torch.tensor(state, device=device).unsqueeze(0)\n",
    "    return state_tensor\n",
    "    \n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.counter = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        #saving memory\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "        state_tensor = convert_state(state)\n",
    "        \n",
    "        if next_state is None:\n",
    "            state_tensor_next = None            \n",
    "        else:\n",
    "            state_tensor_next = convert_state(next_state)\n",
    "            \n",
    "        action_tensor = torch.tensor([action], device=device).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device).unsqueeze(0)/100. # reward scaling\n",
    "        self.memory[self.position] = Transition(state_tensor, action_tensor, state_tensor_next, reward)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f2f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q networks of the game, there are several networks in this part, but the first one is currently working\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, size_hidden, output_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, size_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(size_hidden)\n",
    "        self.fc2 = nn.Linear(size_hidden, size_hidden)   \n",
    "        self.bn2 = nn.BatchNorm1d(size_hidden)\n",
    "        self.fc3 = nn.Linear(size_hidden, size_hidden)  \n",
    "        self.bn3 = nn.BatchNorm1d(size_hidden)\n",
    "        self.fc4 = nn.Linear(size_hidden, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        h1 = F.relu(self.bn1(self.fc1(x.float())))\n",
    "        h2 = F.relu(self.bn2(self.fc2(h1)))\n",
    "        h3 = F.relu(self.bn3(self.fc3(h2)))\n",
    "        output = self.fc4(h3.view(h3.size(0), -1))\n",
    "        return output\n",
    "\n",
    "class DQN_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, numChannels = 1):\n",
    "        # call the parent constructor\n",
    "        super().__init__()\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=6,\n",
    "        kernel_size=(3, 3))\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels=12,\n",
    "        kernel_size=(3, 3))\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=800, out_features=50)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = nn.Linear(in_features=50, out_features=7)\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = F.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d399c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_SIZE = 42    #size of the state\n",
    "HIDDEN_SIZE = 64    #hidden nodes of the network\n",
    "ACTION_SIZE = 7     #number of actiones and nodes in the last layer\n",
    "\n",
    "# Q_network = NewDQN(OBS_SIZE, ACTION_SIZE).to(device)\n",
    "# Q_target = NewDQN(OBS_SIZE, ACTION_SIZE).to(device)\n",
    "# Q_target.load_state_dict(Q_network.state_dict())\n",
    "# Q_target.eval()\n",
    "\n",
    "Q_network = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "Q_target = DQN(OBS_SIZE, HIDDEN_SIZE, ACTION_SIZE).to(device)\n",
    "Q_target.load_state_dict(Q_network.state_dict())\n",
    "Q_target.eval()\n",
    "\n",
    "# Q_network = DQN_CNN().to(device)\n",
    "# Q_target = DQN_CNN().to(device)\n",
    "# Q_target.load_state_dict(Q_network.state_dict())\n",
    "# Q_target.eval()\n",
    "\n",
    "\n",
    "#updating Q_target network after a certain epoch\n",
    "TARGET_UPDATE = 20\n",
    "\n",
    "optimizer = optim.SGD(Q_network.parameters(), lr=0.01)\n",
    "memory = ReplayMemory(40000)    #capacity of the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a77cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_Greedy_Policy():\n",
    "    \n",
    "    def __init__(self, epsilon, decay, min_epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay = decay\n",
    "        self.epsilon_min = min_epsilon\n",
    "                \n",
    "    def __call__(self, state):\n",
    "        is_greedy = random() > self.epsilon\n",
    "        if is_greedy :        # we select greedy action\n",
    "            with torch.no_grad():\n",
    "                Q_network.eval()\n",
    "                # index of the maximum over dimension 1.\n",
    "                index_action = Q_network(state).max(1)[1].view(1, 1).cpu()[0][0].item()\n",
    "                Q_network.train()\n",
    "        else:\n",
    "            index_action = randint(0,6)     # we sample a random action\n",
    "        return index_action\n",
    "                \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon*self.decay\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        \n",
    "    def reset(self):\n",
    "        self.epsilon = self.epsilon_start\n",
    "\n",
    "#creating the policy, with specific factors        \n",
    "policy = E_Greedy_Policy(0.99, decay=0.999, min_epsilon=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58a1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.8\n",
    "\n",
    "def optimize_model():\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    # print(non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q values using policy net\n",
    "    Q_values = Q_network(state_batch).gather(1, action_batch)\n",
    "    # Compute next Q values using Q_targets\n",
    "    next_Q_values = torch.zeros( BATCH_SIZE, device=device)\n",
    "    next_Q_values[non_final_mask] = Q_target(non_final_next_states).max(1)[0].detach()\n",
    "    next_Q_values = next_Q_values.unsqueeze(1)\n",
    "    \n",
    "    # Compute targets\n",
    "    target_Q_values = (next_Q_values * GAMMA) + reward_batch\n",
    "    # Compute MSE Loss\n",
    "    loss = F.mse_loss(Q_values, target_Q_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Trick: gradient clipping\n",
    "    for param in Q_network.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48d5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with the warmup\n",
      "games per 100: 87\n",
      "Episode  0 :  reward : 100.0 eps:  0.98901  loss: tensor(0.6194)\n",
      "average rewards in previous 10 games: 10.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_26897/2040808974.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_26897/2584003766.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, network)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wz/clvs5gsd7jl5jxf044nvdvw00000gn/T/ipykernel_26897/2584003766.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "\n",
    "policy.reset()\n",
    "game = connect4()\n",
    "game.reset()\n",
    "\n",
    "rewards_history = []\n",
    "\n",
    "# Warmup phase!\n",
    "memory_filled = False\n",
    "\n",
    "while not memory_filled:\n",
    "    game = connect4()\n",
    "    state = game.state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get action and act in the world\n",
    "        state_tensor = convert_state(state)\n",
    "        action = policy(state_tensor)\n",
    "        next_state, reward, done = game.step(action, Q_target)\n",
    "        total_reward += float(reward)\n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = game.reset()\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, float(reward))\n",
    "        state = next_state\n",
    "\n",
    "    memory_filled = memory.capacity == len(memory)\n",
    "\n",
    "print('Done with the warmup')\n",
    "    \n",
    "for i_episode in range(num_episodes):\n",
    "    # New dungeon at every run\n",
    "    game = connect4()\n",
    "    state = game.state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get action and act in the world\n",
    "        state_tensor = convert_state(state)\n",
    "        action = policy(state_tensor)\n",
    "        next_state, reward, done = game.step(action, Q_target)\n",
    "        total_reward += float(reward)\n",
    "        \n",
    "        # Observe new state\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, float(reward))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization\n",
    "        started_training = True\n",
    "\n",
    "        l = optimize_model()\n",
    "\n",
    "    policy.update_epsilon()\n",
    "    rewards_history.append( float(total_reward) )\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        Q_target.load_state_dict(Q_network.state_dict())\n",
    "    \n",
    "    if (i_episode) % 1000 == 0:\n",
    "        game.Q = Q_target\n",
    "        print('win games per 100:',game.win_p100(Q_target))\n",
    "        print('Episode ', i_episode, ': ', 'reward :',  total_reward, 'eps: ', \n",
    "              policy.epsilon, ' loss:', l.detach().cpu())   \n",
    "        print( 'average rewards in previous 10 games:',sum(rewards_history[-10:])/10)    \n",
    "\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4287de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test1 = connect4()\n",
    "test1.play_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a845e39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test2 = connect4()\n",
    "test2.play_game(starter = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ce2cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_DQN_1 = connect4()\n",
    "test_DQN_1.play_DQN(Q_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
